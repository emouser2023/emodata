--------------------------------------------------------------------------------
                     working dir: /media/sdb_access/Emotion_multi_model_CLIP/EXP/clip_hmdb_32_frame/ViT-B/16/hmdb51/HMDB_training
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
                               Config
{   'data': {   'base_json_path': '/media/sdb_access/Sania_ACM_Neurips/dataset_samples_code/clip_json_test/',
                'base_video_path': '/media/sdb_access/Sania_ACM_Neurips/dataset_samples_code/test_videos_clips/',
                'batch_size': 5,
                'dataset': 'hmdb51',
                'gpus': 2,
                'image_tmpl': 'img_{:05d}.jpg',
                'index_bias': 1,
                'input_size': 224,
                'label_list': '/media/sdb_access/Emotion_multi_model_CLIP/lists/hmdb51_labels.csv',
                'modality': 'RGB',
                'num_classes': 51,
                'num_segments': 32,
                'randaug': {'M': 0, 'N': 0},
                'root_path': '/media/sdb_access/HMDB_51/all_video',
                'seg_length': 1,
                'split': 1,
                'train_list': '/media/sdb_access/Sania_ACM_Neurips/dataset_samples_code/clip_json_test_list.txt',
                'val_list': '/media/sdb_access/Emotion_multi_model_CLIP/dataset_splits/HMDB/Zero_shot/test.txt',
                'workers': 8},
    'logging': {'eval_freq': 1, 'print_freq': 10},
    'network': {   'arch': 'ViT-B/16',
                   'describe': None,
                   'drop_out': 0.0,
                   'emb_dropout': 0.0,
                   'fix_img': False,
                   'fix_text': False,
                   'init': True,
                   'sim_header': 'Transf',
                   'type': 'clip_hmdb_32_frame'},
    'pretrain': None,
    'resume': None,
    'seed': 1024,
    'solver': {   'clip_gradient': 20,
                  'epoch_offset': 0,
                  'epochs': 50,
                  'evaluate': False,
                  'f_ratio': 10,
                  'loss_type': 'nll',
                  'lr': 5e-06,
                  'lr_decay_factor': 0.1,
                  'lr_decay_step': 15,
                  'lr_warmup_step': 5,
                  'momentum': 0.9,
                  'optim': 'adamw',
                  'ratio': 1,
                  'start_epoch': 0,
                  'type': 'cosine',
                  'weight_decay': 0.2},
    'training_name': 'HMDB_training',
    'weight_save_dir': '/media/sdb_access/Emotion_multi_model_CLIP/EXP'}
--------------------------------------------------------------------------------
train transforms: [Compose(
    <datasets.transforms_ss.GroupMultiScaleCrop object at 0x7fd2f86a6e80>
    <datasets.transforms_ss.GroupRandomHorizontalFlip object at 0x7fd2f86a6d00>
    <datasets.transforms_ss.GroupRandomColorJitter object at 0x7fd2f86a6e20>
    <datasets.transforms_ss.GroupRandomGrayscale object at 0x7fd2f86a6d90>
    <datasets.transforms_ss.GroupGaussianBlur object at 0x7fd2f86a6cd0>
    <datasets.transforms_ss.GroupSolarization object at 0x7fd2f86a6bb0>
), Compose(
    <datasets.transforms_ss.Stack object at 0x7fd2f86a6b20>
    <datasets.transforms_ss.ToTorchFormatTensor object at 0x7fd2f86a6c70>
    <datasets.transforms_ss.GroupNormalize object at 0x7fd2f86a6c10>
)]
val transforms: [Compose(
    <datasets.transforms_ss.GroupScale object at 0x7fd2f86a68e0>
    <datasets.transforms_ss.GroupCenterCrop object at 0x7fd2f86a67f0>
), Compose(
    <datasets.transforms_ss.Stack object at 0x7fd2f86a6580>
    <datasets.transforms_ss.ToTorchFormatTensor object at 0x7fd2f86a66a0>
    <datasets.transforms_ss.GroupNormalize object at 0x7fd2f86a6640>
