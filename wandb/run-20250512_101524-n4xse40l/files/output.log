--------------------------------------------------------------------------------
                     working dir: /home/cvpr_phd_1/2_Emotion_multi_model_CLIP-main/Emotion_multi_model_CLIP-main/EXP/clip_8_frame/ViT-B/16/emotion_recog/Emotion_training
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
                               Config
{   'data': {   'base_json_path': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/json_folder/',
                'base_video_path': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/main_dataset_folder',
                'batch_size': 10,
                'dataset': 'emotion_recog',
                'gpus': 2,
                'index_bias': 1,
                'input_size': 224,
                'modality': 'RGB',
                'num_segments': 8,
                'number_of_class': 7,
                'randaug': {'M': 0, 'N': 0},
                'seg_length': 1,
                'split': 1,
                'test_base_json_path': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/json_folder/',
                'test_base_video_path': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/main_dataset_folder',
                'test_list': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/Final_dataset_list/filter_dataset/all_3_sets/split_test_final_list.txt',
                'train_list': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/Final_dataset_list/filter_dataset/all_3_sets/split_train_final_list.txt',
                'workers': 8},
    'eval_data': {   'base_json_path_dev': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/json/dev',
                     'base_json_path_test': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/json/test',
                     'base_json_path_train': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/json/train',
                     'base_video_path': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/',
                     'batch_size': 10,
                     'dev_list': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/txt/dev_list.txt',
                     'test_list': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/txt/test_list.txt',
                     'train_list': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/txt/train_list.txt'},
    'logging': {'eval_freq': 1, 'print_freq': 10},
    'network': {   'arch': 'ViT-B/16',
                   'describe': None,
                   'drop_out': 0.0,
                   'emb_dropout': 0.0,
                   'fix_img': False,
                   'fix_text': False,
                   'init': True,
                   'sim_header': 'Transf',
                   'type': 'clip_8_frame'},
    'pretrain': None,
    'resume': None,
    'seed': 1024,
    'solver': {   'clip_gradient': 20,
                  'epoch_offset': 0,
                  'epochs': 50,
                  'evaluate': False,
                  'f_ratio': 10,
                  'loss_type': 'nll',
                  'lr': 5e-06,
                  'lr_decay_factor': 0.1,
                  'lr_decay_step': 15,
                  'lr_warmup_step': 5,
                  'momentum': 0.9,
                  'optim': 'adamw',
                  'ratio': 1,
                  'start_epoch': 0,
                  'type': 'cosine',
                  'weight_decay': 0.2},
    'training_name': 'Emotion_training',
    'weight_save_dir': '/home/cvpr_phd_1/2_Emotion_multi_model_CLIP-main/Emotion_multi_model_CLIP-main/EXP'}
--------------------------------------------------------------------------------
dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
loading clip pretrained model!
/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
train transforms: [Compose(
    <datasets.transforms_ss.GroupMultiScaleCrop object at 0x7f8c6c5e2310>
    <datasets.transforms_ss.GroupRandomHorizontalFlip object at 0x7f8da9b0ac40>
    <datasets.transforms_ss.GroupRandomColorJitter object at 0x7f8da9b0afd0>
    <datasets.transforms_ss.GroupRandomGrayscale object at 0x7f8da9b0af70>
    <datasets.transforms_ss.GroupGaussianBlur object at 0x7f8da9b0ad00>
    <datasets.transforms_ss.GroupSolarization object at 0x7f8da9b0ae20>
), Compose(
    <datasets.transforms_ss.Stack object at 0x7f8da9b0ae80>
    <datasets.transforms_ss.ToTorchFormatTensor object at 0x7f8da9b0acd0>
    <datasets.transforms_ss.GroupNormalize object at 0x7f8da9b0ac70>
)]
val transforms: [Compose(
    <datasets.transforms_ss.GroupScale object at 0x7f8da9b0ab20>
    <datasets.transforms_ss.GroupCenterCrop object at 0x7f8da9b0aac0>
), Compose(
    <datasets.transforms_ss.Stack object at 0x7f8da9b0aa60>
    <datasets.transforms_ss.ToTorchFormatTensor object at 0x7f8da9b0a8e0>
    <datasets.transforms_ss.GroupNormalize object at 0x7f8da9b0a880>
)]
/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torchaudio/functional/functional.py:571: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (513) may be set too low.
  warnings.warn(
CLIP_model Trainable Parameters: 8.681M
Audio model Trainable Parameters: 0.656M
5e-06
5e-06
5e-06
5e-05
AdamW
/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Epoch:0  iteration:0/2140, total loss:1.945312, lr:0.000000
Epoch:0  iteration:10/2140, total loss:1.934570, lr:0.000000
Epoch:0  iteration:20/2140, total loss:1.991211, lr:0.000000
Epoch:0  iteration:30/2140, total loss:1.972656, lr:0.000000
Epoch:0  iteration:40/2140, total loss:1.984375, lr:0.000000
Epoch:0  iteration:50/2140, total loss:1.923828, lr:0.000000
Epoch:0  iteration:60/2140, total loss:1.910156, lr:0.000000
Epoch:0  iteration:70/2140, total loss:1.940430, lr:0.000000
Epoch:0  iteration:80/2140, total loss:1.922852, lr:0.000000
Epoch:0  iteration:90/2140, total loss:1.991211, lr:0.000000
Epoch:0  iteration:100/2140, total loss:1.972656, lr:0.000000
Epoch:0  iteration:110/2140, total loss:1.962891, lr:0.000000
Epoch:0  iteration:120/2140, total loss:1.959961, lr:0.000000
Epoch:0  iteration:130/2140, total loss:1.928711, lr:0.000000
Epoch:0  iteration:140/2140, total loss:1.959961, lr:0.000000
Epoch:0  iteration:150/2140, total loss:1.916992, lr:0.000000
Epoch:0  iteration:160/2140, total loss:1.944336, lr:0.000000
Epoch:0  iteration:170/2140, total loss:1.935547, lr:0.000000
Epoch:0  iteration:180/2140, total loss:1.943359, lr:0.000000
Epoch:0  iteration:190/2140, total loss:1.935547, lr:0.000000
Epoch:0  iteration:200/2140, total loss:1.967773, lr:0.000000
Epoch:0  iteration:210/2140, total loss:1.949219, lr:0.000000
Epoch:0  iteration:220/2140, total loss:1.934570, lr:0.000000
Epoch:0  iteration:230/2140, total loss:1.948242, lr:0.000000
Epoch:0  iteration:240/2140, total loss:1.949219, lr:0.000000
Epoch:0  iteration:250/2140, total loss:1.949219, lr:0.000000
Epoch:0  iteration:260/2140, total loss:1.927734, lr:0.000000
Epoch:0  iteration:270/2140, total loss:1.958984, lr:0.000000
Epoch:0  iteration:280/2140, total loss:1.935547, lr:0.000000
Epoch:0  iteration:290/2140, total loss:1.964844, lr:0.000000
Epoch:0  iteration:300/2140, total loss:1.937500, lr:0.000000
Epoch:0  iteration:310/2140, total loss:1.962891, lr:0.000000
Epoch:0  iteration:320/2140, total loss:1.957031, lr:0.000000
Epoch:0  iteration:330/2140, total loss:1.904297, lr:0.000000
Epoch:0  iteration:340/2140, total loss:1.952148, lr:0.000000
Epoch:0  iteration:350/2140, total loss:1.947266, lr:0.000000
Epoch:0  iteration:360/2140, total loss:1.920898, lr:0.000000
Epoch:0  iteration:370/2140, total loss:1.935547, lr:0.000000
Epoch:0  iteration:380/2140, total loss:1.910156, lr:0.000000
Epoch:0  iteration:390/2140, total loss:1.944336, lr:0.000000
Epoch:0  iteration:400/2140, total loss:1.932617, lr:0.000000
Epoch:0  iteration:410/2140, total loss:1.935547, lr:0.000000
Epoch:0  iteration:420/2140, total loss:1.916016, lr:0.000000
Epoch:0  iteration:430/2140, total loss:1.917969, lr:0.000000
Epoch:0  iteration:440/2140, total loss:1.885742, lr:0.000000
Epoch:0  iteration:450/2140, total loss:1.891602, lr:0.000000
Epoch:0  iteration:460/2140, total loss:1.996094, lr:0.000000
Epoch:0  iteration:470/2140, total loss:1.933594, lr:0.000000
Traceback (most recent call last):
  File "train_new.py", line 364, in <module>
    main()
  File "train_new.py", line 304, in main
    clip.model.convert_weights(model)
  File "/home/cvpr_phd_1/1_split_Emotion_multi_model_CLIP-main/clip/model.py", line 385, in convert_weights
    model.apply(_convert_weights_to_fp16)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 726, in apply
    module.apply(fn)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 726, in apply
    module.apply(fn)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 726, in apply
    module.apply(fn)
  [Previous line repeated 3 more times]
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in apply
    fn(self)
  File "/home/cvpr_phd_1/1_split_Emotion_multi_model_CLIP-main/clip/model.py", line 369, in _convert_weights_to_fp16
    l.weight.data = l.weight.data.half()
KeyboardInterrupt
