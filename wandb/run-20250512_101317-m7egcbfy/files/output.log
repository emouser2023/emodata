--------------------------------------------------------------------------------
                     working dir: /home/cvpr_phd_1/2_Emotion_multi_model_CLIP-main/Emotion_multi_model_CLIP-main/EXP/clip_8_frame/ViT-B/16/emotion_recog/Emotion_training
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
                               Config
{   'data': {   'base_json_path': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/json_folder/',
                'base_video_path': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/main_dataset_folder',
                'batch_size': 15,
                'dataset': 'emotion_recog',
                'gpus': 2,
                'index_bias': 1,
                'input_size': 224,
                'modality': 'RGB',
                'num_segments': 8,
                'number_of_class': 7,
                'randaug': {'M': 0, 'N': 0},
                'seg_length': 1,
                'split': 1,
                'test_base_json_path': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/json_folder/',
                'test_base_video_path': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/main_dataset_folder',
                'test_list': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/Final_dataset_list/filter_dataset/all_3_sets/split_test_final_list.txt',
                'train_list': '/home/cvpr_phd_1/1_new_dataset/movie_parts_20/Final_dataset_list/filter_dataset/all_3_sets/split_train_final_list.txt',
                'workers': 8},
    'eval_data': {   'base_json_path_dev': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/json/dev',
                     'base_json_path_test': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/json/test',
                     'base_json_path_train': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/json/train',
                     'base_video_path': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/',
                     'batch_size': 15,
                     'dev_list': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/txt/dev_list.txt',
                     'test_list': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/txt/test_list.txt',
                     'train_list': '/home/cvpr_phd_1/MELD.Raw/MELD_dataset/txt/train_list.txt'},
    'logging': {'eval_freq': 1, 'print_freq': 10},
    'network': {   'arch': 'ViT-B/16',
                   'describe': None,
                   'drop_out': 0.0,
                   'emb_dropout': 0.0,
                   'fix_img': False,
                   'fix_text': False,
                   'init': True,
                   'sim_header': 'Transf',
                   'type': 'clip_8_frame'},
    'pretrain': None,
    'resume': None,
    'seed': 1024,
    'solver': {   'clip_gradient': 20,
                  'epoch_offset': 0,
                  'epochs': 50,
                  'evaluate': False,
                  'f_ratio': 10,
                  'loss_type': 'nll',
                  'lr': 5e-06,
                  'lr_decay_factor': 0.1,
                  'lr_decay_step': 15,
                  'lr_warmup_step': 5,
                  'momentum': 0.9,
                  'optim': 'adamw',
                  'ratio': 1,
                  'start_epoch': 0,
                  'type': 'cosine',
                  'weight_decay': 0.2},
    'training_name': 'Emotion_training',
    'weight_save_dir': '/home/cvpr_phd_1/2_Emotion_multi_model_CLIP-main/Emotion_multi_model_CLIP-main/EXP'}
--------------------------------------------------------------------------------
dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
loading clip pretrained model!
/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
train transforms: [Compose(
    <datasets.transforms_ss.GroupMultiScaleCrop object at 0x7fd1cad0e310>
    <datasets.transforms_ss.GroupRandomHorizontalFlip object at 0x7fd306236d00>
    <datasets.transforms_ss.GroupRandomColorJitter object at 0x7fd306236fa0>
    <datasets.transforms_ss.GroupRandomGrayscale object at 0x7fd306236fd0>
    <datasets.transforms_ss.GroupGaussianBlur object at 0x7fd306236e80>
    <datasets.transforms_ss.GroupSolarization object at 0x7fd306236e20>
), Compose(
    <datasets.transforms_ss.Stack object at 0x7fd306236d30>
    <datasets.transforms_ss.ToTorchFormatTensor object at 0x7fd306236ca0>
    <datasets.transforms_ss.GroupNormalize object at 0x7fd306236ac0>
)]
val transforms: [Compose(
    <datasets.transforms_ss.GroupScale object at 0x7fd306236b20>
    <datasets.transforms_ss.GroupCenterCrop object at 0x7fd306236a60>
), Compose(
    <datasets.transforms_ss.Stack object at 0x7fd306236880>
    <datasets.transforms_ss.ToTorchFormatTensor object at 0x7fd3062368e0>
    <datasets.transforms_ss.GroupNormalize object at 0x7fd306236820>
)]
/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torchaudio/functional/functional.py:571: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (64) may be set too high. Or, the value for `n_freqs` (513) may be set too low.
  warnings.warn(
CLIP_model Trainable Parameters: 8.681M
Audio model Trainable Parameters: 0.656M
5e-06
5e-06
5e-06
5e-05
AdamW
/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Traceback (most recent call last):
  File "train_new.py", line 364, in <module>
    main()
  File "train_new.py", line 279, in main
    audio_embedding = model_audio(audios)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 169, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/cvpr_phd_1/1_split_Emotion_multi_model_CLIP-main/modules/Audio_encoder.py", line 321, in forward
    features, _ = self.cnn(audio)  # (batch_size, seq_len, 768)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torchaudio/models/wav2vec2/model.py", line 119, in forward
    x, lengths = self.feature_extractor(waveforms, lengths)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torchaudio/models/wav2vec2/components.py", line 139, in forward
    x, length = layer(x, length)  # (batch, feature, frame)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torchaudio/models/wav2vec2/components.py", line 88, in forward
    x = self.conv(x)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 313, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/home/cvpr_phd_1/.conda/envs/action_clip/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 309, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.38 GiB (GPU 0; 47.53 GiB total capacity; 11.68 GiB already allocated; 1.32 GiB free; 11.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
